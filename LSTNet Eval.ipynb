{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on https://github.com/apache/incubator-mxnet/tree/master/example/multivariate_time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-12-14 09:33:40--  https://github.com/laiguokun/multivariate-time-series-data/raw/master/electricity/electricity.txt.gz\n",
      "Resolving github.com (github.com)... 140.82.118.3, 140.82.118.4\n",
      "Connecting to github.com (github.com)|140.82.118.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/laiguokun/multivariate-time-series-data/master/electricity/electricity.txt.gz [following]\n",
      "--2018-12-14 09:33:40--  https://raw.githubusercontent.com/laiguokun/multivariate-time-series-data/master/electricity/electricity.txt.gz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.16.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.16.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 17993794 (17M) [application/octet-stream]\n",
      "Saving to: ‘electricity.txt.gz’\n",
      "\n",
      "electricity.txt.gz  100%[===================>]  17.16M  5.16MB/s    in 3.5s    \n",
      "\n",
      "2018-12-14 09:33:44 (4.96 MB/s) - ‘electricity.txt.gz’ saved [17993794/17993794]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir lst-data && \\\n",
    " cd lst-data && \\\n",
    " wget https://github.com/laiguokun/multivariate-time-series-data/raw/master/electricity/electricity.txt.gz && \\\n",
    " gunzip electricity.txt.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'metrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-576ba9272d37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'metrics'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mxnet as mx\n",
    "import argparse\n",
    "import logging\n",
    "import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data'\n",
    "max_records = None\n",
    "q = 24*7\n",
    "horizon = 3\n",
    "splits = '0.6,0.2'\n",
    "batch_size = 128\n",
    "filter_list = '6,12,18' # unique filter sizes\n",
    "num_filters = 100 # number of each filter size\n",
    "recurrent_state_size = 100 # number of hidden units in each unrolled recurrent cell\n",
    "seasonal_period = 24 # time between seasonal measurements\n",
    "time_interval = 1 # time between each measurement\n",
    "gpus = '' # list of gpus to run, e.g. 0 or 0,2,5. empty means using cpu. \n",
    "optimizer = 'adam'\n",
    "lr = 0.001\n",
    "dropout = 0.2\n",
    "num_epochs = 100\n",
    "save_period = 20 # save checkpoint for every n epochs\n",
    "model_prefix = 'electricity_model' # prefix for saving model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_iters(data_dir, max_records, q, horizon, splits, batch_size):\n",
    "    \"\"\"\n",
    "    Load & generate training examples from multivariate time series data\n",
    "    :return: data iters & variables required to define network architecture\n",
    "    \"\"\"\n",
    "    # Read in data as numpy array\n",
    "    df = pd.read_csv(os.path.join(data_dir, \"electricity.txt\"), sep=\",\", header=None)\n",
    "    feature_df = df.iloc[:, :].astype(float)\n",
    "    x = feature_df.as_matrix()\n",
    "    x = x[:max_records] if max_records else x\n",
    "\n",
    "    # Construct training examples based on horizon and window\n",
    "    x_ts = np.zeros((x.shape[0] - q, q, x.shape[1]))\n",
    "    y_ts = np.zeros((x.shape[0] - q, x.shape[1]))\n",
    "    for n in range(x.shape[0]):\n",
    "        if n + 1 < q:\n",
    "            continue\n",
    "        elif n + 1 + horizon > x.shape[0]:\n",
    "            continue\n",
    "        else:\n",
    "            y_n = x[n + horizon, :]\n",
    "            x_n = x[n + 1 - q:n + 1, :]\n",
    "        x_ts[n-q] = x_n\n",
    "        y_ts[n-q] = y_n\n",
    "\n",
    "    # Split into training and testing data\n",
    "    training_examples = int(x_ts.shape[0] * splits[0])\n",
    "    valid_examples = int(x_ts.shape[0] * splits[1])\n",
    "    x_train, y_train = x_ts[:training_examples], \\\n",
    "                       y_ts[:training_examples]\n",
    "    x_valid, y_valid = x_ts[training_examples:training_examples + valid_examples], \\\n",
    "                       y_ts[training_examples:training_examples + valid_examples]\n",
    "    x_test, y_test = x_ts[training_examples + valid_examples:], \\\n",
    "                     y_ts[training_examples + valid_examples:]\n",
    "\n",
    "    #build iterators to feed batches to network\n",
    "    train_iter = mx.io.NDArrayIter(data=x_train,\n",
    "                                   label=y_train,\n",
    "                                   batch_size=batch_size)\n",
    "    val_iter = mx.io.NDArrayIter(data=x_valid,\n",
    "                                 label=y_valid,\n",
    "                                 batch_size=batch_size)\n",
    "    test_iter = mx.io.NDArrayIter(data=x_test,\n",
    "                                  label=y_test,\n",
    "                                  batch_size=batch_size)\n",
    "    return train_iter, val_iter, test_iter\n",
    "\n",
    "def sym_gen(train_iter, q, filter_list, num_filter, dropout, rcells, skiprcells, seasonal_period, time_interval):\n",
    "\n",
    "    input_feature_shape = train_iter.provide_data[0][1]\n",
    "    X = mx.symbol.Variable(train_iter.provide_data[0].name)\n",
    "    Y = mx.sym.Variable(train_iter.provide_label[0].name)\n",
    "\n",
    "    # reshape data before applying convolutional layer (takes 4D shape incase you ever work with images)\n",
    "    conv_input = mx.sym.reshape(data=X, shape=(0, 1, q, -1))\n",
    "\n",
    "    ###############\n",
    "    # CNN Component\n",
    "    ###############\n",
    "    outputs = []\n",
    "    for i, filter_size in enumerate(filter_list):\n",
    "        # pad input array to ensure number output rows = number input rows after applying kernel\n",
    "        padi = mx.sym.pad(data=conv_input, mode=\"constant\", constant_value=0,\n",
    "                          pad_width=(0, 0, 0, 0, filter_size - 1, 0, 0, 0))\n",
    "        convi = mx.sym.Convolution(data=padi, kernel=(filter_size, input_feature_shape[2]), num_filter=num_filter)\n",
    "        acti = mx.sym.Activation(data=convi, act_type='relu')\n",
    "        trans = mx.sym.reshape(mx.sym.transpose(data=acti, axes=(0, 2, 1, 3)), shape=(0, 0, 0))\n",
    "        outputs.append(trans)\n",
    "    cnn_features = mx.sym.Concat(*outputs, dim=2)\n",
    "    cnn_reg_features = mx.sym.Dropout(cnn_features, p=dropout)\n",
    "\n",
    "    ###############\n",
    "    # RNN Component\n",
    "    ###############\n",
    "    stacked_rnn_cells = mx.rnn.SequentialRNNCell()\n",
    "    for i, recurrent_cell in enumerate(rcells):\n",
    "        stacked_rnn_cells.add(recurrent_cell)\n",
    "        stacked_rnn_cells.add(mx.rnn.DropoutCell(dropout))\n",
    "    outputs, states = stacked_rnn_cells.unroll(length=q, inputs=cnn_reg_features, merge_outputs=False)\n",
    "    rnn_features = outputs[-1] #only take value from final unrolled cell for use later\n",
    "\n",
    "    ####################\n",
    "    # Skip-RNN Component\n",
    "    ####################\n",
    "    stacked_rnn_cells = mx.rnn.SequentialRNNCell()\n",
    "    for i, recurrent_cell in enumerate(skiprcells):\n",
    "        stacked_rnn_cells.add(recurrent_cell)\n",
    "        stacked_rnn_cells.add(mx.rnn.DropoutCell(dropout))\n",
    "    outputs, states = stacked_rnn_cells.unroll(length=q, inputs=cnn_reg_features, merge_outputs=False)\n",
    "\n",
    "    # Take output from cells p steps apart\n",
    "    p = int(seasonal_period / time_interval)\n",
    "    output_indices = list(range(0, q, p))\n",
    "    outputs.reverse()\n",
    "    skip_outputs = [outputs[i] for i in output_indices]\n",
    "    skip_rnn_features = mx.sym.concat(*skip_outputs, dim=1)\n",
    "\n",
    "    ##########################\n",
    "    # Autoregressive Component\n",
    "    ##########################\n",
    "    auto_list = []\n",
    "    for i in list(range(input_feature_shape[2])):\n",
    "        time_series = mx.sym.slice_axis(data=X, axis=2, begin=i, end=i+1)\n",
    "        fc_ts = mx.sym.FullyConnected(data=time_series, num_hidden=1)\n",
    "        auto_list.append(fc_ts)\n",
    "    ar_output = mx.sym.concat(*auto_list, dim=1)\n",
    "\n",
    "    ######################\n",
    "    # Prediction Component\n",
    "    ######################\n",
    "    neural_components = mx.sym.concat(*[rnn_features, skip_rnn_features], dim=1)\n",
    "    neural_output = mx.sym.FullyConnected(data=neural_components, num_hidden=input_feature_shape[2])\n",
    "    model_output = neural_output + ar_output\n",
    "    loss_grad = mx.sym.LinearRegressionOutput(data=model_output, label=Y)\n",
    "    return loss_grad, [v.name for v in train_iter.provide_data], [v.name for v in train_iter.provide_label]\n",
    "\n",
    "def train(symbol, train_iter, valid_iter, data_names, label_names):\n",
    "    devs = mx.cpu() if args.gpus is None or args.gpus is '' else [mx.gpu(int(i)) for i in args.gpus.split(',')]\n",
    "    module = mx.mod.Module(symbol, data_names=data_names, label_names=label_names, context=devs)\n",
    "    module.bind(data_shapes=train_iter.provide_data, label_shapes=train_iter.provide_label)\n",
    "    module.init_params(mx.initializer.Uniform(0.1))\n",
    "    module.init_optimizer(optimizer=args.optimizer, optimizer_params={'learning_rate': args.lr})\n",
    "\n",
    "    for epoch in range(1, args.num_epochs+1):\n",
    "        train_iter.reset()\n",
    "        val_iter.reset()\n",
    "        for batch in train_iter:\n",
    "            module.forward(batch, is_train=True)  # compute predictions\n",
    "            module.backward()  # compute gradients\n",
    "            module.update() # update parameters\n",
    "\n",
    "        train_pred = module.predict(train_iter).asnumpy()\n",
    "        train_label = train_iter.label[0][1].asnumpy()\n",
    "        print('\\nMetrics: Epoch %d, Training %s' % (epoch, metrics.evaluate(train_pred, train_label)))\n",
    "\n",
    "        val_pred = module.predict(val_iter).asnumpy()\n",
    "        val_label = val_iter.label[0][1].asnumpy()\n",
    "        print('Metrics: Epoch %d, Validation %s' % (epoch, metrics.evaluate(val_pred, val_label)))\n",
    "\n",
    "        if epoch % args.save_period == 0 and epoch > 1:\n",
    "            module.save_checkpoint(prefix=os.path.join(\"../models/\", args.model_prefix), epoch=epoch, save_optimizer_states=False)\n",
    "        if epoch == args.num_epochs:\n",
    "            module.save_checkpoint(prefix=os.path.join(\"../models/\", args.model_prefix), epoch=epoch, save_optimizer_states=False)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build data iterators\n",
    "train_iter, val_iter, test_iter = build_iters(data_dir, max_records, q, horizon, splits, batch_size)\n",
    "\n",
    "# Choose cells for recurrent layers: each cell will take the output of the previous cell in the list\n",
    "rcells = [mx.rnn.GRUCell(num_hidden = recurrent_state_size)]\n",
    "skiprcells = [mx.rnn.LSTMCell(num_hidden = recurrent_state_size)]\n",
    "\n",
    "# Define network symbol\n",
    "symbol, data_names, label_names = sym_gen(train_iter, q, filter_list, num_filters,\n",
    "                                          dropout, rcells, skiprcells, seasonal_period, time_interval)\n",
    "\n",
    "# train cnn model\n",
    "train(symbol, train_iter, val_iter, data_names, label_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
